Continuando il discorso sulle IDS, teoria di Machine learing applicato al networking

Per poter scegliere l'approccio migliore(misuse o anomaly), occorre fare una prima analisi del dataset.
Se mi aspetto che il comportamento futuro rimanga simile a quello attuale, voglio un dataset completo con ogni tipologia di attacco e addestrare su quello per fare misuse detection.
Se ho dataset sbilanciati questo approccio non funziona, l'algoritmo non riesce ad apprendere bene come sia fatto un attacco, allora posso pensare di usare solo la porzione di traffico normale
e addestrare per fare anomaly detection(che funziona pure meglio contro zero day attacks).

La separazione tra le classi che fa IDS avviene sulla base di una metrica misurabile scelta a priori, si lavora con le pdf del traffico benigno/anomalo

Alcuni questioni:  
    -come decido la threshold tra le due classi? non possiamo farlo a caso quindi occorre un meccanismo per trovarla
    -applico la threshold indiscriminatamente a tutti gli utenti? non dovrei
    -ogni quanto si aggiorna la threshold?
    -in tutto questo, che prestazioni offre un metodo con soglia? un sola soglia è poco robusto,
    posso pensare di creare un messanismo multisoglia con pericolosità incrementale, cosi posso in qualche modo 'filtrare' le anomalie

Attacchi noti che vogliamo individuare:
    probe: scansione di rete per acquisire informazioni
    r2l: dall'esterno cercano di entrare nei nodi interni alla rete
    u2r: si cerca di passare da privilegi utenti a privilegi root(in una escalation che segue la scaletta)
    dos

Dato che si lavora con meccanismi data-driven, la scelta delle feature è importante, devo utilizzare solo quelle significative.
Tra queste i timestamp saranno sempre importanti, tutti i miei dati sono delle serie temporali di pacchetti.
Altre feature importanti sono: durata della sessione, tipo di protocollo utilizzato, flag, i byte inviati, errori nei pacchetti, byte urg
Va fatta EDA per ridurre il più possibile la ridondanza nella feature e tenere le migliori senza perdere informazione(altrimenti avrei troppe feature a considerare ogni cosa ed esplode la complessità)

Modelli regressivi utili per predirre il futuro se ci sono pattern ciclici nel traffico
Altri approcci usano reti neurali(per lavorare su un singolo parametroo più di uno, sono abbastanza potenti per poterlo fare),
 SMV(per trovare la boundry tra le classi) o alberi decisionali(nelle loro varie versioni)

Performance 
accuracy = cosa ho predetto bene/tutto quanto
precision = di quello che ho predetto vero/ quanto è veramente vero? TP/TP+FP
recall =  di quello che è veramente vero, quando ho predetto bene? TP/TP+FN

Cerchiamo di mantenere FP e FN il più bassi possibile

Altre Performance che possiamo utilizzare, metriche statische, solitamente sono un pò semplicistiche ma sono facilmente spiegabili, proprietà importante per noi, aiuta a comprendere costa sta accadendo
Media 'classica' o mobile(per tener conto del tempo)

Possiamo assumere che i nostri parametri abbiano distribuzione Gaussiana(pdf), cosa che ci aiuta a fare ipotesi sulla loro media/varianza.
la Media però è un parametro polarizzato verso i valori molto grandi o piccoli.
la mediana invece è meno polarizzata, 50% campioni più piccoli e 50% più grandi, non importa quanto siano più piccoli o più grandi.
posso definire la deviazione dalla mediana.

Goodness-of-fit
Cerco delle distribuzioni note che funzionino per spiegare i miei dati -> posso cercare una pdf congiunta.
nel caso di gaussiana multivariata, dove ho più parametri gaussiani, analizzando i grafici/curve di livello delle pdf posso prendere delle decisioni sulla posizione della threshold per la classificazione

Alberi decisionali

molto usati nelle reti perchè sono molto spiegabili e hanno buone prestazioni.
Utilizzano feature numeriche, cosa che funziona bene per la maggior parte dei dati del networking, se abbiamo feature continue possiamo definire degli intervalli significativi e renderla categorica/numerica.
Si comincia andando a costruire l'albero, scegliendo il nodo radice. 
A questo sarà associata la feature più informativa(basandosi sul concetto di entropia della informazione).
Trovato il nodo root, ci muoviamo verso i figli sulla base del valore assunto dal campione per quella feature.
Si continua così muovendosi lungo l'albero finchè non si raggiunge un nodo foglia, a cui è associata la classificazione da dare al campione

Come troviamo le feature da associare ad ogni nodo dell'albero?

Calcoliamo:
D = Dataset
A = Attributi
P(w_j) = Probabilità che un campione appartenga ad una certa classe, stimata a partire dal dataset come num.occorrenze di un classe/num.totale campioni 
D(v) = Dataset dove l'attributo A assume il valore v
i(D(v)) = incertezza associata al Dataset D(v), numericamente è uguale all'entropia

Entropia del Dataset E(D)= -Σ (P(w_j) * log2(P(w_j)))
						(j∈Dataset) 
 
Guadagno G(D, A) = E(D) -Σ (|D(v)| / |D|) * i(D(v))
		           (v∈values(A))

Ogni volta sceglieremo la feature con il maggiore guadagno, in modo da ridurre l'entropia il più possibile fino ad arrivare ad una certezza.

Problemi degli alberi:
    -Overfitting, risolto andando a fare operazioni di 'pruning', taglio dei pezzi agli alberi dopo l'addestramento, o durante l'addestramento arresto un pò prima l'esecuzione dell'algoritmo cosi da ridurre l'overfitting(meccanismo di validazione per scegliere i tagli migliori)
    -Molto sensibili agli errori, occorre un dataset 'pulito'
    -instabilità, lo abbiamo visto a lezione, se cambia un campione può cambiare l'intera struttura dell'albero, mostra come l'albero rimane sempre legato ai dati, la capacità di astrazione rimane limitata

Un altro vantaggio degli alberi decisionali è che si traducono molto bene in regole iptables, permettondoci di implementarlo come regole sequenziali dove man mano verifico il valore delle varie features

Random Forest
Vogliono tenere tutti i vantaggi degli alberi ma risolvendone i problemi, bypassa i problemi di overfitting e instabilità utilizzando più alberi.
ogni albero ha un numero di feature che è massimo pari alla radice del num. totale delle feature, e si addestra solo su una porzione del dataset(campionamento con sostituzione, n campioni totali ma solo d sono diversi, gli altri sono loro copie, per il meccanismo di entropia)0
La classificazione avviene come una votazione per maggioranza tra le classificazioni di tutti gli alberi. Quello che guadagno in prestazioni e risoluzione problemi lo perdo però in spiegabilità, diventa tutto più oscuro a noi .
Ogni addestramento può essere eseguito in parallelo, migliorando ancora di più le prestazioni

Reti neurali

Algoritmi di machine learning che cercano di emulare il funzionamento dei neuroni umani.
Nella versione più semplice abbiamo:
    -layer di input, dove si trovano tutte le feature
    -hidden layer, dove si trovano tutti neuroni, ognuno è legate agli input tramite degli archi prestazioni
    -l'output è ottenuto come combinazione delle funzioni di attivazione dei neuroni(funz. non lineari)

Solitamente vogliamo algoritmi spiegabili per giustificare le nostre scelte.
Ma gli algoritmi visti (KNN, SVM, Alberi decisionali o Random Forest) per quanto sono spiegabili hanno dei problemi:
    -KNN è il migliore per distinguere gli attacchi tra di loro, ma ha delle prestazioni troppo lente
    -gli altri algoritmi sono più performanti computazionalmente, ma hanno performance peggiori a causa della confusione tra attacchi probe e dos

Le reti neurali ci permettono di risolvere questi problemi, abbiamo comunque una fase di training pesante per determinare tutti i vari parametri, ma in fase di testing dobbiamo fare solo calcoli matriciali molto veloci,
e grazie al gran numero di parametri abbiamo un modello con una grande capacità espressiva, permette al modello di trovare aspetti più fini del problema.
Questo ragionamento viene estremizzato quando parliamo di deep neural network, dove si ha un numero di hidden >= 3.

A causa di tutta questa potenza espressiva però le NN tendono ad overfittare il dataset, perciò si devono usare tecniche simili a quelle usate dalla random forest(dataset ridotto e feature ridotte).
Vanishing gradient: dato che la maggior parte delle funzioni di attivazione tendono a saturare, il gradiente tende ad azzerarsi, portando l'algoritmo a non convergere.

%addestro autoencoder solo su taffico benigno(riconosciuto dalla label, tecnica supervisionata)
%ora la rete neurale sa cosa sia traffico normale e sa riconoscere bene quale sia il traffico non normale -> attacchi
%una volta trovati i campioni di attacco, come li distinguo tra di loro? KNN sembra l'algoritmo che separa meglio gli attacchi probe dai DoS
%usare un autoencoder come classificare traffico benigno/anomalo ci protegge anche da zero day attacks meglio di altri algoritmi tipo alberi decisionali o random forest, che hanno meno capacità di astrazione